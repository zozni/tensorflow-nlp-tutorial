{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "memory network (kor).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlviU4N-NAv1"
      },
      "source": [
        "2021년 10월 8일에 최종 테스트 되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtP8ZuteNnXu"
      },
      "source": [
        "링크 : https://wikidocs.net/82475"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jJWHK_eANDN0",
        "outputId": "aa946e96-864e-4328-ad7a-eac9e105bfbd"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktt34R7EGRZM"
      },
      "source": [
        "# 메모리 네트워크를 이용한 한국어 QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq0aHOs0GWCP"
      },
      "source": [
        "데이터는 각각 아래의 링크에서 다운로드 할 수 있습니다.  \n",
        "훈련 데이터 : https://bit.ly/31SqtHy  \n",
        "테스트 데이터 : https://bit.ly/3f7rH5g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9XiEZRvGXJJ"
      },
      "source": [
        "## 단어 사전 등록이 간편한 형태소 분석기 customized_konlpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "Us3UZp-tQPmT",
        "outputId": "cd573911-4ddc-4ff2-f901-14e41a3ca595"
      },
      "source": [
        "pip install customized_konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: customized_konlpy in /usr/local/lib/python3.6/dist-packages (0.0.64)\n",
            "Requirement already satisfied: Jpype1>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from customized_konlpy) (0.7.5)\n",
            "Requirement already satisfied: konlpy>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from customized_konlpy) (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (1.18.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (0.4.3)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy>=0.4.4->customized_konlpy) (4.6.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.12.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (2020.6.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.4.4->customized_konlpy) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEgY4ShzQudC"
      },
      "source": [
        "from ckonlpy.tag import Twitter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HHNd8PuTQoQK",
        "outputId": "77716e12-9d05-4204-95e0-e98a4abf2d2f"
      },
      "source": [
        "twitter = Twitter()\n",
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
            ]
          },
          "execution_count": 101,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "3Oke4Hd8Qr0r",
        "outputId": "4dd39abb-4cf5-45c5-b98f-7ffb3a4b7cb7"
      },
      "source": [
        "twitter.add_dictionary('은경이', 'Noun')\n",
        "twitter.morphs('은경이는 사무실로 갔습니다.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['은경이', '는', '사무실', '로', '갔습니다', '.']"
            ]
          },
          "execution_count": 102,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDopEhHUGb7C"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bkg6sr9QQMy"
      },
      "source": [
        "from ckonlpy.tag import Twitter\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from nltk import FreqDist\n",
        "from functools import reduce\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4A77lkgQFzJ"
      },
      "source": [
        "TRAIN_FILE = os.path.join(\"qa1_single-supporting-fact_train_kor.txt\")\n",
        "TEST_FILE = os.path.join(\"qa1_single-supporting-fact_test_kor.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Cm6WlUGdkr"
      },
      "source": [
        "## Babi 데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "H0RfBIooRJqq",
        "outputId": "282ca1c3-0316-45a3-c1fd-bf562861f899"
      },
      "source": [
        "i = 0\n",
        "lines = open(TRAIN_FILE , \"rb\")\n",
        "for line in lines:\n",
        "    line = line.decode(\"utf-8\").strip()\n",
        "    # lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
        "    i = i + 1\n",
        "    print(line)\n",
        "    if i == 20:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 필웅이는 화장실로 갔습니다.\n",
            "2 은경이는 복도로 이동했습니다.\n",
            "3 필웅이는 어디야? \t화장실\t1\n",
            "4 수종이는 복도로 복귀했습니다.\n",
            "5 경임이는 정원으로 갔습니다.\n",
            "6 수종이는 어디야? \t복도\t4\n",
            "7 은경이는 사무실로 갔습니다.\n",
            "8 경임이는 화장실로 뛰어갔습니다.\n",
            "9 수종이는 어디야? \t복도\t4\n",
            "10 필웅이는 복도로 갔습니다.\n",
            "11 수종이는 사무실로 가버렸습니다.\n",
            "12 수종이는 어디야? \t사무실\t11\n",
            "13 은경이는 정원으로 복귀했습니다.\n",
            "14 은경이는 침실로 갔습니다.\n",
            "15 경임이는 어디야? \t화장실\t8\n",
            "1 경임이는 사무실로 가버렸습니다.\n",
            "2 경임이는 화장실로 이동했습니다.\n",
            "3 경임이는 어디야? \t화장실\t2\n",
            "4 필웅이는 침실로 이동했습니다.\n",
            "5 수종이는 복도로 갔습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMiWML_tGfLY"
      },
      "source": [
        "## 스토리, 질문, 답변 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_IwuvrBRNeA"
      },
      "source": [
        "def read_data(dir):\n",
        "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
        "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
        "    lines = open(dir, \"rb\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.decode(\"utf-8\") # b' 제거\n",
        "        line = line.strip() # '\\n' 제거\n",
        "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
        "        # 여기까지는 모든 줄에 적용되는 전처리\n",
        "\n",
        "        if int(idx) == 1:\n",
        "            story_temp = []\n",
        "\n",
        "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
        "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
        "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "\n",
        "        else: # 현재 읽는 줄이 스토리인 경우\n",
        "            story_temp.append(text) # 임시 저장\n",
        "\n",
        "    lines.close()\n",
        "    return stories, questions, answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RabIlPFPWgaa"
      },
      "source": [
        "train_data = read_data(TRAIN_FILE)\n",
        "test_data = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGPZuNgMRQHg"
      },
      "source": [
        "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
        "test_stories, test_questions, test_answers = read_data(TEST_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "enIOcAh2RRbw",
        "outputId": "343516d3-ae0e-4316-cecd-7774a4a030e3"
      },
      "source": [
        "print('훈련용 스토리의 개수 :', len(train_stories))\n",
        "print('훈련용 질문의 개수 :',len(train_questions))\n",
        "print('훈련용 답변의 개수 :',len(train_answers))\n",
        "print('테스트용 스토리의 개수 :',len(test_stories))\n",
        "print('테스트용 질문의 개수 :',len(test_questions))\n",
        "print('테스트용 답변의 개수 :',len(test_answers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련용 스토리의 개수 : 10000\n",
            "훈련용 질문의 개수 : 10000\n",
            "훈련용 답변의 개수 : 10000\n",
            "테스트용 스토리의 개수 : 1000\n",
            "테스트용 질문의 개수 : 1000\n",
            "테스트용 답변의 개수 : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "D8Wm1KHjRSjI",
        "outputId": "a955ddfa-671a-4480-ce1b-d84a1ffc9c3f"
      },
      "source": [
        "train_stories[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['은경이는 부엌으로 가버렸습니다.',\n",
              " '필웅이는 사무실로 가버렸습니다.',\n",
              " '수종이는 복도로 뛰어갔습니다.',\n",
              " '은경이는 사무실로 복귀했습니다.',\n",
              " '경임이는 사무실로 이동했습니다.',\n",
              " '경임이는 침실로 갔습니다.']"
            ]
          },
          "execution_count": 111,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Thg7HE61RT14",
        "outputId": "2f8fdaf3-5d82-4a26-e799-07b1564b445c"
      },
      "source": [
        "train_questions[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'은경이는 어디야? '"
            ]
          },
          "execution_count": 112,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "a_bB2m_HRXIA",
        "outputId": "e6fab148-0e26-4a3a-c4df-5ce7ed7a942e"
      },
      "source": [
        "train_answers[3572]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'사무실'"
            ]
          },
          "execution_count": 113,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnDC9aq2GiPZ"
      },
      "source": [
        "## 단어 집합 생성 및 토큰화 및 스토리와 질문의 최대 길이 구하기  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD2OBre7GzQZ"
      },
      "source": [
        "이제 토큰화 함수를 정의하고, 이로부터 Vocabulary를 생성하는 함수를 만들어봅시다. 아래의 함수는 영어 데이터셋에 사용했던 토큰화 함수와 동일합니다. 현재는 한국어이므로 아래의 토큰화 함수를 그대로 사용하는 것은 바람직하지는 않지만, 임시로 사용해보겠습니다. 어절 단위로 했을 때 어떤 단어들이 있는지 출력해보기 위함입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJGkCdQ4WBRi"
      },
      "source": [
        "def tokenize(sent):\n",
        "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiMTed3WWCD7"
      },
      "source": [
        "def preprocess_data(train_data, test_data):\n",
        "    counter = FreqDist()\n",
        "\n",
        "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    # 각 샘플의 길이를 저장하는 리스트\n",
        "    story_len = []\n",
        "    question_len = []\n",
        "\n",
        "    for stories, questions, answers in [train_data, test_data]:\n",
        "        for story in stories:\n",
        "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
        "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
        "            for word in stories: # 단어 집합에 단어 추가\n",
        "                counter[word] += 1\n",
        "        for question in questions:\n",
        "            question = tokenize(question)\n",
        "            question_len.append(len(question))\n",
        "            for word in question:\n",
        "                counter[word] += 1\n",
        "        for answer in answers:\n",
        "            answer = tokenize(answer)\n",
        "            for word in answer:\n",
        "                counter[word] += 1\n",
        "\n",
        "    # 단어 집합 생성\n",
        "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
        "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
        "\n",
        "    # 가장 긴 샘플의 길이\n",
        "    story_max_len = np.max(story_len)\n",
        "    question_max_len = np.max(question_len)\n",
        "\n",
        "    return word2idx, idx2word, story_max_len, question_max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dlGJpYGuqH"
      },
      "source": [
        "word2idx를 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UMQLgV1oWF1k",
        "outputId": "98b1d9c7-59d0-46dc-ebaf-3377141a818d"
      },
      "source": [
        "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3erH8vSjWjsP",
        "outputId": "25329c95-05dc-4c61-8dfc-a849ad884793"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'.': 1, '경임이는': 2, '은경이는': 3, '수종이는': 4, '필웅이는': 5, '이동했습니다': 6, '가버렸습니다': 7, '뛰어갔습니다': 8, '복귀했습니다': 9, '갔습니다': 10, '화장실로': 11, '정원으로': 12, '복도로': 13, '어디야': 14, '?': 15, '부엌으로': 16, '사무실로': 17, '침실로': 18, '화장실': 19, '정원': 20, '사무실': 21, '침실': 22, '복도': 23, '부엌': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-6vL5XG6Im"
      },
      "source": [
        "띄어쓰기 단위, 다시 말해 어절 단위로 했을 때 나오는 총 토큰의 수는 24개입니다. 19번 토큰부터 24번 토큰까지를 봤을 때 장소에 해당되는 명사들은 '화장실', '정원', '사무실', '침실', '복도', '부엌'이 있는 것 같습니다. 그렇다면, 11번 토큰부터 19번 토큰 사이에 등장하는 '화장실로', '정원으로', '복도로', '부엌으로', '사무실로', '침실로'로 분리된 토큰들은 형태소 분석을 하였을 때 전부 '화장실', '정원', '사무실', '침실', '복도', '부엌'으로 분리되어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYXcFUZTGsBT"
      },
      "source": [
        "## 형태소 분석기 사전 등록"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlwR3nwaXQwo"
      },
      "source": [
        "'화장실로, 정원으로, 복도로, 부엌으로, 사무실로, 침실로'가 형태소 분석기가 정상적으로 분리하는지 확인이 필요함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "i2RE7DQ9RYMY",
        "outputId": "f17256c2-3e19-4704-f06e-10001e38b491"
      },
      "source": [
        "twitter = Twitter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "A0F7PjuyRtew",
        "outputId": "77f1e77a-a572-419e-b825-80ad58707d9a"
      },
      "source": [
        "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
        "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
        "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
        "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
        "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
        "print(twitter.morphs('은경이는 침실로 갔습니다.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['은', '경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
            "['경', '임', '이', '는', '정원', '으로', '가버렸습니다', '.']\n",
            "['수종', '이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
            "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
            "['수종', '이', '는', '사무실', '로', '갔습니다', '.']\n",
            "['은', '경이', '는', '침실', '로', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsYWIKfRVsKS"
      },
      "source": [
        "twitter.add_dictionary('은경이', 'Noun')\n",
        "twitter.add_dictionary('경임이', 'Noun')\n",
        "twitter.add_dictionary('수종이', 'Noun')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "uYUFcbCBbbA-",
        "outputId": "50f45174-66ec-4700-86ae-aae279b36157"
      },
      "source": [
        "print(twitter.morphs('은경이는 화장실로 이동했습니다.'))\n",
        "print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))\n",
        "print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))\n",
        "print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))\n",
        "print(twitter.morphs('수종이는 사무실로 갔습니다.'))\n",
        "print(twitter.morphs('은경이는 침실로 갔습니다.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['은경이', '는', '화장실', '로', '이동', '했습니다', '.']\n",
            "['경임이', '는', '정원', '으로', '가버렸습니다', '.']\n",
            "['수종이', '는', '복도', '로', '뛰어갔습니다', '.']\n",
            "['필웅이', '는', '부엌', '으로', '복귀', '했습니다', '.']\n",
            "['수종이', '는', '사무실', '로', '갔습니다', '.']\n",
            "['은경이', '는', '침실', '로', '갔습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSLbfiHgHCF5"
      },
      "source": [
        "##단어 집합 생성 및 토큰화 및 스토리와 질문의 최대 길이 구하기(다시)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJr8X5lPV0DP"
      },
      "source": [
        "def tokenize(sent):\n",
        "    return twitter.morphs(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY_OqDtUbLMZ"
      },
      "source": [
        "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "3j5FIQQGbRFI",
        "outputId": "61b67bab-69df-459f-ee5b-2b52206ec7a5"
      },
      "source": [
        "print(word2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'는': 1, '.': 2, '로': 3, '했습니다': 4, '으로': 5, '경임이': 6, '은경이': 7, '수종이': 8, '필웅이': 9, '이동': 10, '가버렸습니다': 11, '뛰어갔습니다': 12, '복귀': 13, '화장실': 14, '정원': 15, '복도': 16, '갔습니다': 17, '사무실': 18, '부엌': 19, '침실': 20, '어디': 21, '야': 22, '?': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "mCDO93srbtav",
        "outputId": "5e68eee5-3f63-455a-f54a-407e07737c89"
      },
      "source": [
        "idx2word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{1: '는',\n",
              " 2: '.',\n",
              " 3: '로',\n",
              " 4: '했습니다',\n",
              " 5: '으로',\n",
              " 6: '경임이',\n",
              " 7: '은경이',\n",
              " 8: '수종이',\n",
              " 9: '필웅이',\n",
              " 10: '이동',\n",
              " 11: '가버렸습니다',\n",
              " 12: '뛰어갔습니다',\n",
              " 13: '복귀',\n",
              " 14: '화장실',\n",
              " 15: '정원',\n",
              " 16: '복도',\n",
              " 17: '갔습니다',\n",
              " 18: '사무실',\n",
              " 19: '부엌',\n",
              " 20: '침실',\n",
              " 21: '어디',\n",
              " 22: '야',\n",
              " 23: '?'}"
            ]
          },
          "execution_count": 82,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sTflyTBRb0UH",
        "outputId": "8bec16b5-d79d-4b3a-c75b-6fb057155775"
      },
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oWnDG29FcFzW",
        "outputId": "4a3a7cd1-320e-4995-dfdc-eea0c359c98b"
      },
      "source": [
        "print('스토리의 최대 길이 :',story_max_len)\n",
        "print('질문의 최대 길이 :',question_max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스토리의 최대 길이 : 70\n",
            "질문의 최대 길이 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwwzIsKmHH66"
      },
      "source": [
        "## 정수 인코딩 및 패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVtQY63LcG6W"
      },
      "source": [
        "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
        "    Xs, Xq, Y = [], [], []\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "\n",
        "    stories, questions, answers = data\n",
        "    for story, question, answer in zip(stories, questions, answers):\n",
        "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
        "        xq = [word2idx[w] for w in tokenize(question)]\n",
        "        Xs.append(xs)\n",
        "        Xq.append(xq)\n",
        "        Y.append(word2idx[answer])\n",
        "\n",
        "        # 스토리와 질문은 각각의 최대 길이로 패딩\n",
        "        # 정답은 원-핫 인코딩\n",
        "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
        "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
        "           to_categorical(Y, num_classes=len(word2idx) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8toMbSHcISP"
      },
      "source": [
        "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
        "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "puLdisufcJVu",
        "outputId": "7b13564d-16b0-446e-e91f-d1836917b214"
      },
      "source": [
        "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 70) (10000, 5) (10000, 24) (1000, 70) (1000, 5) (1000, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvYHNf2tHJwj"
      },
      "source": [
        "## 메모리 네트워크 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey3XSg9AcKhu"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWqGB8WJcLwu"
      },
      "source": [
        "# 에포크 횟수\n",
        "train_epochs = 120\n",
        "# 배치 크기\n",
        "batch_size = 32\n",
        "# 임베딩 크기\n",
        "embed_size = 50\n",
        "# LSTM의 크기\n",
        "lstm_size = 64\n",
        "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
        "dropout_rate = 0.30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KiSgI55icNJu",
        "outputId": "d0ce9b17-ef22-45c9-f37f-522f5180baeb"
      },
      "source": [
        "# 플레이스 홀더. 입력을 담는 변수\n",
        "input_sequence = Input((story_max_len,))\n",
        "question = Input((question_max_len,))\n",
        "\n",
        "print('Stories :', input_sequence)\n",
        "print('Question:', question)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stories : Tensor(\"input_1:0\", shape=(None, 70), dtype=float32)\n",
            "Question: Tensor(\"input_2:0\", shape=(None, 5), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUeW2Jt1cPSV"
      },
      "source": [
        "# 스토리를 위한 첫번째 임베딩. 그림에서의 Embedding A\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=embed_size))\n",
        "input_encoder_m.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, embedding_dim) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
        "\n",
        "# 스토리를 위한 두번째 임베딩. 그림에서의 Embedding C\n",
        "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=question_max_len))\n",
        "input_encoder_c.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvbx-92tcQt-"
      },
      "source": [
        "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=embed_size,\n",
        "                               input_length=question_max_len))\n",
        "question_encoder.add(Dropout(dropout_rate))\n",
        "# 결과 : (samples, question_max_len, embedding_dim) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "E5dctAXZcR6d",
        "outputId": "24e0747b-a9a8-4699-8563-7ef1222b6a96"
      },
      "source": [
        "# 실질적인 임베딩 과정\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)\n",
        "\n",
        "print('Input encoded m', input_encoded_m)\n",
        "print('Input encoded c', input_encoded_c)\n",
        "print('Question encoded', question_encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input encoded m Tensor(\"sequential/Identity:0\", shape=(None, 70, 50), dtype=float32)\n",
            "Input encoded c Tensor(\"sequential_1/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
            "Question encoded Tensor(\"sequential_2/Identity:0\", shape=(None, 5, 50), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "e-L-up6FcVj-",
        "outputId": "02dae2aa-b2ea-44b9-c554-d85ff861498c"
      },
      "source": [
        "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
        "# 유사도는 내적을 사용한다.\n",
        "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
        "match = Activation('softmax')(match)\n",
        "print('Match shape', match)\n",
        "# 결과 : (samples, story_maxlen, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이\n",
        "\n",
        "# add the match matrix with the second input vector sequence\n",
        "response = add([match, input_encoded_c])  # (samples, story_max_len, question_max_len)\n",
        "response = Permute((2, 1))(response)  # (samples, question_max_len, story_max_len)\n",
        "print('Response shape', response)\n",
        "\n",
        "# concatenate the response vector with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])\n",
        "print('Answer shape', answer)\n",
        "\n",
        "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
        "answer = Dropout(dropout_rate)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
        "# we output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match shape Tensor(\"activation/Identity:0\", shape=(None, 70, 5), dtype=float32)\n",
            "Response shape Tensor(\"permute/Identity:0\", shape=(None, 5, 70), dtype=float32)\n",
            "Answer shape Tensor(\"concatenate/Identity:0\", shape=(None, 5, 120), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EW5px9B4cYUV",
        "outputId": "9ee7cf2d-2c52-4f85-82b8-356dd28e9a10"
      },
      "source": [
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# start training the model\n",
        "history = model.fit([Xstrain, Xqtrain],\n",
        "         Ytrain, batch_size, train_epochs,\n",
        "         validation_data=([Xstest, Xqtest], Ytest))\n",
        "\n",
        "# save model\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         multiple             1200        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       (None, 5, 50)        1200        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 70, 5)        0           sequential[1][0]                 \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 70, 5)        0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             120         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 70, 5)        0           activation[0][0]                 \n",
            "                                                                 sequential_1[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 5, 70)        0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 120)       0           permute[0][0]                    \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 64)           47360       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 24)           1560        dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 24)           0           dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 51,440\n",
            "Trainable params: 51,440\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/120\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 1.8982 - acc: 0.1693 - val_loss: 1.7868 - val_acc: 0.2470\n",
            "Epoch 2/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.7075 - acc: 0.2567 - val_loss: 1.6049 - val_acc: 0.3870\n",
            "Epoch 3/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.5579 - acc: 0.3737 - val_loss: 1.4619 - val_acc: 0.4140\n",
            "Epoch 4/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.4848 - acc: 0.4171 - val_loss: 1.4182 - val_acc: 0.4510\n",
            "Epoch 5/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.4464 - acc: 0.4446 - val_loss: 1.4419 - val_acc: 0.4200\n",
            "Epoch 6/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.4159 - acc: 0.4557 - val_loss: 1.3817 - val_acc: 0.4640\n",
            "Epoch 7/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.3700 - acc: 0.4781 - val_loss: 1.3279 - val_acc: 0.5040\n",
            "Epoch 8/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.3283 - acc: 0.4939 - val_loss: 1.2875 - val_acc: 0.5190\n",
            "Epoch 9/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.3025 - acc: 0.5037 - val_loss: 1.2792 - val_acc: 0.5020\n",
            "Epoch 10/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2779 - acc: 0.5098 - val_loss: 1.2808 - val_acc: 0.4930\n",
            "Epoch 11/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2613 - acc: 0.5132 - val_loss: 1.2507 - val_acc: 0.5360\n",
            "Epoch 12/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2470 - acc: 0.5163 - val_loss: 1.2421 - val_acc: 0.5180\n",
            "Epoch 13/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2273 - acc: 0.5204 - val_loss: 1.2253 - val_acc: 0.5200\n",
            "Epoch 14/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2214 - acc: 0.5276 - val_loss: 1.2376 - val_acc: 0.5270\n",
            "Epoch 15/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2049 - acc: 0.5250 - val_loss: 1.2172 - val_acc: 0.5220\n",
            "Epoch 16/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.2003 - acc: 0.5222 - val_loss: 1.2091 - val_acc: 0.5170\n",
            "Epoch 17/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1844 - acc: 0.5322 - val_loss: 1.2151 - val_acc: 0.5180\n",
            "Epoch 18/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1694 - acc: 0.5303 - val_loss: 1.2158 - val_acc: 0.5210\n",
            "Epoch 19/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.1671 - acc: 0.5376 - val_loss: 1.2055 - val_acc: 0.5280\n",
            "Epoch 20/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1515 - acc: 0.5399 - val_loss: 1.2079 - val_acc: 0.5250\n",
            "Epoch 21/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1460 - acc: 0.5387 - val_loss: 1.2003 - val_acc: 0.5140\n",
            "Epoch 22/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1382 - acc: 0.5410 - val_loss: 1.2025 - val_acc: 0.5140\n",
            "Epoch 23/120\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 1.1280 - acc: 0.5433 - val_loss: 1.1976 - val_acc: 0.5100\n",
            "Epoch 24/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1137 - acc: 0.5499 - val_loss: 1.2152 - val_acc: 0.5300\n",
            "Epoch 25/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1129 - acc: 0.5511 - val_loss: 1.2064 - val_acc: 0.5190\n",
            "Epoch 26/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.1060 - acc: 0.5502 - val_loss: 1.2110 - val_acc: 0.5210\n",
            "Epoch 27/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0963 - acc: 0.5571 - val_loss: 1.2037 - val_acc: 0.5250\n",
            "Epoch 28/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0846 - acc: 0.5529 - val_loss: 1.2028 - val_acc: 0.5170\n",
            "Epoch 29/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0789 - acc: 0.5555 - val_loss: 1.2014 - val_acc: 0.5100\n",
            "Epoch 30/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0681 - acc: 0.5614 - val_loss: 1.2123 - val_acc: 0.5170\n",
            "Epoch 31/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0644 - acc: 0.5656 - val_loss: 1.2151 - val_acc: 0.5170\n",
            "Epoch 32/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0516 - acc: 0.5719 - val_loss: 1.2093 - val_acc: 0.5210\n",
            "Epoch 33/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0395 - acc: 0.5749 - val_loss: 1.2333 - val_acc: 0.5160\n",
            "Epoch 34/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0355 - acc: 0.5745 - val_loss: 1.2217 - val_acc: 0.5180\n",
            "Epoch 35/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0319 - acc: 0.5769 - val_loss: 1.2105 - val_acc: 0.5160\n",
            "Epoch 36/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0250 - acc: 0.5721 - val_loss: 1.2358 - val_acc: 0.5150\n",
            "Epoch 37/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.0151 - acc: 0.5820 - val_loss: 1.2152 - val_acc: 0.5130\n",
            "Epoch 38/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 1.0022 - acc: 0.5903 - val_loss: 1.2352 - val_acc: 0.5190\n",
            "Epoch 39/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9950 - acc: 0.5869 - val_loss: 1.2399 - val_acc: 0.4980\n",
            "Epoch 40/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9837 - acc: 0.5938 - val_loss: 1.2542 - val_acc: 0.5020\n",
            "Epoch 41/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9710 - acc: 0.6014 - val_loss: 1.2608 - val_acc: 0.4990\n",
            "Epoch 42/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9587 - acc: 0.6077 - val_loss: 1.2451 - val_acc: 0.5060\n",
            "Epoch 43/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.9472 - acc: 0.6113 - val_loss: 1.2598 - val_acc: 0.5020\n",
            "Epoch 44/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9454 - acc: 0.6126 - val_loss: 1.2911 - val_acc: 0.4950\n",
            "Epoch 45/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9342 - acc: 0.6181 - val_loss: 1.2848 - val_acc: 0.4920\n",
            "Epoch 46/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.9166 - acc: 0.6267 - val_loss: 1.2639 - val_acc: 0.5150\n",
            "Epoch 47/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.8987 - acc: 0.6323 - val_loss: 1.2585 - val_acc: 0.5180\n",
            "Epoch 48/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.8820 - acc: 0.6481 - val_loss: 1.2005 - val_acc: 0.5480\n",
            "Epoch 49/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.8306 - acc: 0.6789 - val_loss: 1.1183 - val_acc: 0.6080\n",
            "Epoch 50/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.7546 - acc: 0.7191 - val_loss: 0.9644 - val_acc: 0.6590\n",
            "Epoch 51/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.6478 - acc: 0.7680 - val_loss: 0.8642 - val_acc: 0.6960\n",
            "Epoch 52/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.5953 - acc: 0.7882 - val_loss: 0.7975 - val_acc: 0.7250\n",
            "Epoch 53/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5391 - acc: 0.8089 - val_loss: 0.7459 - val_acc: 0.7310\n",
            "Epoch 54/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.5151 - acc: 0.8147 - val_loss: 0.7087 - val_acc: 0.7480\n",
            "Epoch 55/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4799 - acc: 0.8273 - val_loss: 0.6861 - val_acc: 0.7570\n",
            "Epoch 56/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.4362 - acc: 0.8423 - val_loss: 0.6764 - val_acc: 0.7580\n",
            "Epoch 57/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.4055 - acc: 0.8540 - val_loss: 0.6075 - val_acc: 0.7880\n",
            "Epoch 58/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3672 - acc: 0.8681 - val_loss: 0.5702 - val_acc: 0.7950\n",
            "Epoch 59/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3476 - acc: 0.8743 - val_loss: 0.5195 - val_acc: 0.8060\n",
            "Epoch 60/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.3306 - acc: 0.8822 - val_loss: 0.5008 - val_acc: 0.8180\n",
            "Epoch 61/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.4881 - val_acc: 0.8210\n",
            "Epoch 62/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2943 - acc: 0.8948 - val_loss: 0.4893 - val_acc: 0.8200\n",
            "Epoch 63/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2711 - acc: 0.9023 - val_loss: 0.4696 - val_acc: 0.8310\n",
            "Epoch 64/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2590 - acc: 0.9074 - val_loss: 0.4844 - val_acc: 0.8270\n",
            "Epoch 65/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2523 - acc: 0.9122 - val_loss: 0.4768 - val_acc: 0.8360\n",
            "Epoch 66/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2380 - acc: 0.9167 - val_loss: 0.4523 - val_acc: 0.8420\n",
            "Epoch 67/120\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.2282 - acc: 0.9203 - val_loss: 0.4842 - val_acc: 0.8310\n",
            "Epoch 68/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2187 - acc: 0.9202 - val_loss: 0.4729 - val_acc: 0.8320\n",
            "Epoch 69/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2094 - acc: 0.9247 - val_loss: 0.4587 - val_acc: 0.8450\n",
            "Epoch 70/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2014 - acc: 0.9281 - val_loss: 0.4453 - val_acc: 0.8560\n",
            "Epoch 71/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1968 - acc: 0.9297 - val_loss: 0.4455 - val_acc: 0.8560\n",
            "Epoch 72/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1782 - acc: 0.9345 - val_loss: 0.4398 - val_acc: 0.8550\n",
            "Epoch 73/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1735 - acc: 0.9378 - val_loss: 0.4628 - val_acc: 0.8540\n",
            "Epoch 74/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1653 - acc: 0.9423 - val_loss: 0.4432 - val_acc: 0.8570\n",
            "Epoch 75/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1595 - acc: 0.9426 - val_loss: 0.4374 - val_acc: 0.8590\n",
            "Epoch 76/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1544 - acc: 0.9452 - val_loss: 0.4240 - val_acc: 0.8610\n",
            "Epoch 77/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1514 - acc: 0.9462 - val_loss: 0.4426 - val_acc: 0.8630\n",
            "Epoch 78/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1356 - acc: 0.9524 - val_loss: 0.4601 - val_acc: 0.8640\n",
            "Epoch 79/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1364 - acc: 0.9504 - val_loss: 0.4300 - val_acc: 0.8650\n",
            "Epoch 80/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1339 - acc: 0.9534 - val_loss: 0.4160 - val_acc: 0.8800\n",
            "Epoch 81/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1264 - acc: 0.9555 - val_loss: 0.4788 - val_acc: 0.8650\n",
            "Epoch 82/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1164 - acc: 0.9595 - val_loss: 0.4635 - val_acc: 0.8620\n",
            "Epoch 83/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1194 - acc: 0.9589 - val_loss: 0.4393 - val_acc: 0.8820\n",
            "Epoch 84/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1023 - acc: 0.9654 - val_loss: 0.4558 - val_acc: 0.8750\n",
            "Epoch 85/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1129 - acc: 0.9615 - val_loss: 0.4342 - val_acc: 0.8820\n",
            "Epoch 86/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.1029 - acc: 0.9653 - val_loss: 0.4146 - val_acc: 0.8840\n",
            "Epoch 87/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0970 - acc: 0.9671 - val_loss: 0.4296 - val_acc: 0.8870\n",
            "Epoch 88/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0982 - acc: 0.9670 - val_loss: 0.4720 - val_acc: 0.8810\n",
            "Epoch 89/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0862 - acc: 0.9709 - val_loss: 0.4282 - val_acc: 0.8880\n",
            "Epoch 90/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0855 - acc: 0.9706 - val_loss: 0.4450 - val_acc: 0.8880\n",
            "Epoch 91/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0950 - acc: 0.9683 - val_loss: 0.4410 - val_acc: 0.8780\n",
            "Epoch 92/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0879 - acc: 0.9688 - val_loss: 0.4145 - val_acc: 0.8930\n",
            "Epoch 93/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0865 - acc: 0.9716 - val_loss: 0.4516 - val_acc: 0.8850\n",
            "Epoch 94/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0792 - acc: 0.9748 - val_loss: 0.4393 - val_acc: 0.8800\n",
            "Epoch 95/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0753 - acc: 0.9749 - val_loss: 0.4709 - val_acc: 0.8910\n",
            "Epoch 96/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0765 - acc: 0.9750 - val_loss: 0.4645 - val_acc: 0.8870\n",
            "Epoch 97/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0679 - acc: 0.9785 - val_loss: 0.4555 - val_acc: 0.8870\n",
            "Epoch 98/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0674 - acc: 0.9767 - val_loss: 0.4529 - val_acc: 0.8950\n",
            "Epoch 99/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0663 - acc: 0.9773 - val_loss: 0.4423 - val_acc: 0.8960\n",
            "Epoch 100/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0636 - acc: 0.9780 - val_loss: 0.4493 - val_acc: 0.8920\n",
            "Epoch 101/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0660 - acc: 0.9770 - val_loss: 0.4228 - val_acc: 0.9000\n",
            "Epoch 102/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0649 - acc: 0.9793 - val_loss: 0.4401 - val_acc: 0.8960\n",
            "Epoch 103/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0566 - acc: 0.9801 - val_loss: 0.4814 - val_acc: 0.8980\n",
            "Epoch 104/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0544 - acc: 0.9828 - val_loss: 0.4836 - val_acc: 0.8960\n",
            "Epoch 105/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0556 - acc: 0.9819 - val_loss: 0.5037 - val_acc: 0.8950\n",
            "Epoch 106/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0553 - acc: 0.9816 - val_loss: 0.4691 - val_acc: 0.8950\n",
            "Epoch 107/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0516 - acc: 0.9830 - val_loss: 0.4764 - val_acc: 0.8950\n",
            "Epoch 108/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0500 - acc: 0.9834 - val_loss: 0.5337 - val_acc: 0.8800\n",
            "Epoch 109/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0528 - acc: 0.9834 - val_loss: 0.4939 - val_acc: 0.8960\n",
            "Epoch 110/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0483 - acc: 0.9845 - val_loss: 0.5018 - val_acc: 0.8910\n",
            "Epoch 111/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0495 - acc: 0.9855 - val_loss: 0.4642 - val_acc: 0.8980\n",
            "Epoch 112/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.5153 - val_acc: 0.8990\n",
            "Epoch 113/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0465 - acc: 0.9870 - val_loss: 0.5266 - val_acc: 0.8990\n",
            "Epoch 114/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0464 - acc: 0.9854 - val_loss: 0.4907 - val_acc: 0.9020\n",
            "Epoch 115/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0417 - acc: 0.9864 - val_loss: 0.5617 - val_acc: 0.8920\n",
            "Epoch 116/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0362 - acc: 0.9881 - val_loss: 0.4869 - val_acc: 0.9030\n",
            "Epoch 117/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0348 - acc: 0.9891 - val_loss: 0.4923 - val_acc: 0.9060\n",
            "Epoch 118/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0401 - acc: 0.9893 - val_loss: 0.5041 - val_acc: 0.9020\n",
            "Epoch 119/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0365 - acc: 0.9871 - val_loss: 0.4627 - val_acc: 0.9050\n",
            "Epoch 120/120\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0397 - acc: 0.9883 - val_loss: 0.5235 - val_acc: 0.9060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Iq7PYePXccGu",
        "outputId": "01a53dc9-46d7-4ac5-c40e-de5ec1e31b1a"
      },
      "source": [
        "# plot accuracy and loss plot\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# labels\n",
        "ytest = np.argmax(Ytest, axis=1)\n",
        "\n",
        "# get predictions\n",
        "Ytest_ = model.predict([Xstest, Xqtest])\n",
        "ytest_ = np.argmax(Ytest_, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "Fq0_LEp9eIA2",
        "outputId": "9451512d-05c8-40aa-eaac-48e305950309"
      },
      "source": [
        "NUM_DISPLAY = 30\n",
        "\n",
        "print(\"{:18}|{:5}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
        "print(39 * \"-\")\n",
        "\n",
        "for i in range(NUM_DISPLAY):\n",
        "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
        "    label = idx2word[ytest[i]]\n",
        "    prediction = idx2word[ytest_[i]]\n",
        "    print(\"{:20}: {:7} {}\".format(question, label, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "질문                |실제값  |예측값\n",
            "---------------------------------------\n",
            "은경이 는 어디 야 ?        : 복도      복도\n",
            "필웅이 는 어디 야 ?        : 화장실     화장실\n",
            "경임이 는 어디 야 ?        : 부엌      부엌\n",
            "경임이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 부엌      부엌\n",
            "경임이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 정원      정원\n",
            "수종이 는 어디 야 ?        : 복도      복도\n",
            "경임이 는 어디 야 ?        : 사무실     복도\n",
            "수종이 는 어디 야 ?        : 사무실     복도\n",
            "필웅이 는 어디 야 ?        : 부엌      부엌\n",
            "필웅이 는 어디 야 ?        : 정원      정원\n",
            "수종이 는 어디 야 ?        : 사무실     사무실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "은경이 는 어디 야 ?        : 부엌      부엌\n",
            "은경이 는 어디 야 ?        : 정원      정원\n",
            "은경이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 사무실     부엌\n",
            "은경이 는 어디 야 ?        : 부엌      정원\n",
            "필웅이 는 어디 야 ?        : 복도      복도\n",
            "은경이 는 어디 야 ?        : 사무실     사무실\n",
            "은경이 는 어디 야 ?        : 사무실     사무실\n",
            "경임이 는 어디 야 ?        : 복도      사무실\n",
            "수종이 는 어디 야 ?        : 침실      침실\n",
            "경임이 는 어디 야 ?        : 침실      침실\n",
            "필웅이 는 어디 야 ?        : 침실      침실\n",
            "수종이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 부엌      부엌\n",
            "수종이 는 어디 야 ?        : 부엌      복도\n"
          ]
        }
      ]
    }
  ]
}